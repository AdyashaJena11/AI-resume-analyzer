# -*- coding: utf-8 -*-
"""AI resme analyzer .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rUzDXKKE3LsIqGq_C0y3uGLEbOBTRkOF
"""

!pip install -q sklearn pandas numpy spacy nltk sentence-transformers scikit-learn joblib gradio
!python -m spacy download en_core_web_sm

import nltk
nltk.download('punkt')
nltk.download('stopwords')

import re
import os
import json
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
from sklearn.metrics.pairwise import cosine_similarity
import joblib
import spacy
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

nlp = spacy.load("en_core_web_sm")

STOPWORDS = set(stopwords.words('english'))

# Small synthetic dataset of resume texts with labels (for demo & training)
data = [
    {"text":"Alice — data scientist with 3 years experience in Python, sklearn, pandas, SQL. Masters in CS. Worked at DataCorp. email: alice@mail.com", "label":"Data Scientist"},
    {"text":"Bob — frontend developer experienced in React, JavaScript, HTML, CSS. 2 years at WebInc. bob@mail.com", "label":"Frontend Developer"},
    {"text":"Carol — machine learning engineer. Hands-on with PyTorch, TensorFlow, model deployment, Docker. 4 years at MLWorks. carol@mail.com", "label":"ML Engineer"},
    {"text":"Dave — backend engineer skilled in Node.js, Express, MongoDB and AWS. 3 years. dave@mail.com", "label":"Backend Developer"},
    {"text":"Eve — full stack dev with React, Node, Python, and SQL. 5 years. eve@mail.com", "label":"Full Stack Developer"},
    # Add more samples to improve performance
]

df = pd.DataFrame(data)
df.head()

# Preprocessing and extraction utilities

email_re = re.compile(r'[\w\.-]+@[\w\.-]+\.\w+')
phone_re = re.compile(r'(\+?\d{1,3}[\s-])?(?:\(?\d{3}\)?[\s-]?)?\d{3}[\s-]?\d{4}')

def clean_text(text):
    text = text.replace('\n',' ').strip()
    # optional: lower, but keep case for NER sometimes
    return re.sub(r'\s+',' ', text)

def extract_email(text):
    m = email_re.search(text)
    return m.group(0) if m else None

def extract_phone(text):
    m = phone_re.search(text)
    return m.group(0) if m else None

def extract_entities(text):
    doc = nlp(text)
    orgs = [ent.text for ent in doc.ents if ent.label_ in ('ORG',)]
    edu = [ent.text for ent in doc.ents if ent.label_ in ('ORG','NORP','GPE','PERSON') and 'University' in ent.text or 'College' in ent.text]  # naive
    # extract PERSON as potential name
    persons = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']
    return {"orgs": orgs, "education": edu, "persons": persons}

def extract_skills(text, skillset=None):
    # naive skill matching: provide a list of skills to check presence
    if skillset is None:
        skillset = ["python","pandas","sql","scikit-learn","tensorflow","pytorch","react","node","docker","aws","html","css","javascript"]
    text_low = text.lower()
    found = [s for s in skillset if s.lower() in text_low]
    return found

# quick test
sample = df.loc[0, "text"]
print(clean_text(sample))
print("email:", extract_email(sample))
print("phone:", extract_phone(sample))
print("entities:", extract_entities(sample))
print("skills:", extract_skills(sample))

# Build TF-IDF vectorizer and add simple skill-count as a numeric feature.
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import FeatureUnion

class SkillCount(BaseEstimator, TransformerMixin):
    def __init__(self, skillset=None):
        self.skillset = skillset or ["python","pandas","sql","scikit-learn","tensorflow","pytorch","react","node","docker","aws","html","css","javascript"]
    def fit(self, X, y=None):
        return self
    def transform(self, X):
        counts = []
        for text in X:
            found = sum(1 for s in self.skillset if s.lower() in text.lower())
            counts.append([found])
        return np.array(counts)

# Vectorizer and pipeline
tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=3000)

from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer

# We'll create a combined pipeline manually:
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression

# Prepare X and y
X = df['text'].apply(clean_text).values
y = df['label'].values

# Simple approach: combine tfidf features and skill-count using FeatureUnion
from sklearn.pipeline import FeatureUnion
from sklearn.preprocessing import FunctionTransformer

tfidf_transformer = Pipeline([
    ('tfidf', tfidf)
])

skill_transformer = Pipeline([
    ('skillcount', SkillCount())
])

feature_union = FeatureUnion([
    ('tfidf', tfidf_transformer),
    ('skills', skill_transformer)
])

pipeline = Pipeline([
    ('features', feature_union),
    ('clf', LogisticRegression(max_iter=1000))
])

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)
pipeline.fit(X_train, y_train)

# Evaluate
preds = pipeline.predict(X_test)
print("Accuracy:", accuracy_score(y_test, preds))
print(classification_report(y_test, preds))

fitted_tfidf = pipeline.named_steps['features'].transformer_list[0][1].named_steps['tfidf']

def ats_score(resume_text, job_keywords):
    # job_keywords: list of important keywords from JD
    found = sum(1 for k in job_keywords if k.lower() in resume_text.lower())
    score = found / max(1, len(job_keywords))
    return float(score)

def resume_jd_similarity(resume, jd_text):
    # use TF-IDF vectors and cosine similarity
    vectors = fitted_tfidf.transform([clean_text(resume), clean_text(jd_text)])
    sim = cosine_similarity(vectors[0], vectors[1])[0][0]
    return float(sim)

# Demo
jd = "Looking for Data Scientist with Python, pandas, SQL, machine learning and model deployment experience."
resume = df.loc[0, "text"]
print("ATS score:", ats_score(resume, ["python","sql","machine learning","pandas","docker"]))
print("Similarity:", resume_jd_similarity(resume, jd))

def analyze_resume(resume_text, job_description=None, job_keywords=None):
    t = clean_text(resume_text)
    email = extract_email(t)
    phone = extract_phone(t)
    entities = extract_entities(t)
    skills = extract_skills(t)
    # classification
    pred_role = pipeline.predict([t])[0]
    # ATS score if keywords provided
    ats = None
    if job_keywords:
        ats = ats_score(t, job_keywords)
    # similarity if JD provided
    sim = None
    if job_description:
        sim = resume_jd_similarity(t, job_description)
    # suggestions: missing top keywords from JD
    suggestions = []
    if job_keywords:
        missing = [k for k in job_keywords if k.lower() not in t.lower()]
        suggestions = missing[:10]
    return {
        "email": email,
        "phone": phone,
        "entities": entities,
        "skills": skills,
        "predicted_role": pred_role,
        "ats_score": ats,
        "similarity": sim,
        "suggestions": suggestions
    }

# Try
print(analyze_resume(df.loc[0,"text"], job_description=jd, job_keywords=["python","sql","machine learning","pandas","docker"]))

# Save the whole pipeline (includes features and classifier)
model_path = "resume_pipeline.joblib"
joblib.dump(pipeline, model_path)
print("Saved model to", model_path)

from sentence_transformers import SentenceTransformer
# choose a small model for Colab memory; this may download ~100-200MB
embed_model = SentenceTransformer('all-MiniLM-L6-v2')

def embed_similarity(resume, jd):
    v1 = embed_model.encode([clean_text(resume)])
    v2 = embed_model.encode([clean_text(jd)])
    return float(cosine_similarity([v1[0]],[v2[0]])[0][0])

# demo
print("Embedding similarity:", embed_similarity(resume, jd))

import gradio as gr
import json # Ensure json is imported

# Assume analyze_resume function is defined elsewhere in the notebook

def gradio_analyze(resume_text, job_desc, keywords_text):
    keywords = [k.strip() for k in keywords_text.split(",")] if keywords_text else None
    out = analyze_resume(resume_text, job_description=job_desc, job_keywords=keywords)

    # Build readable output
    summary = {
        "Predicted Role": out["predicted_role"],
        "Email": out["email"],
        "Phone": out["phone"],
        "Skills Found": ", ".join(out["skills"]),
        "ATS Score (%)": out["ats_score"] * 100 if out["ats_score"] is not None else None,
        "Similarity": out["similarity"],
        "Suggestions (missing keywords)": ", ".join(out["suggestions"])
    }

    # --- AI Feedback Placeholder ---
    # To implement AI feedback, you would typically use a language model here.
    # For example, using the google.generativeai library:
    # import google.generativeai as genai
    # genai.configure(api_key=userdata.get('GOOGLE_API_KEY'))
    # model = genai.GenerativeModel('gemini-pro') # Or another suitable model
    # prompt = f"Analyze the following resume against the job description and provide feedback:\n\nResume:\n{resume_text}\n\nJob Description:\n{job_desc}\n\nFeedback:"
    # try:
    #     response = model.generate_content(prompt)
    #     ai_feedback = response.text
    # except Exception as e:
    #     ai_feedback = f"Error generating feedback: {e}"
    # -----------------------------

    # For now, providing a placeholder message
    ai_feedback = "AI Feedback: Integrate a language model here for detailed suggestions."

    # Print scores to console (as requested previously)
    print("ATS Score (%):", out["ats_score"] * 100 if out["ats_score"] is not None else None)
    print("Similarity:", out["similarity"])

    return json.dumps(summary, indent=2), out["entities"], out["ats_score"] * 100 if out["ats_score"] is not None else None, out["similarity"], ai_feedback

iface = gr.Interface(
    fn=gradio_analyze,
    inputs=[
        gr.Textbox(lines=10, label="Paste Resume Text"),
        gr.Textbox(lines=5, label="Paste Job Description (optional)"),
        gr.Textbox(lines=1, label="Comma-separated Job Keywords (optional)")
    ],
    outputs=[
        gr.Textbox(label="Analysis Summary (JSON)"),
        gr.JSON(label="Named Entities"),
        gr.Number(label="ATS Score (%)"), # Display ATS Score separately as percentage
        gr.Number(label="Similarity Score"), # Display Similarity separately
        gr.Textbox(label="AI Feedback") # Add AI Feedback output
    ],
    examples=[
        [df.loc[0,"text"], jd, "python,sql,pandas,machine learning"]
    ],
    title="Resume Analyzer (Local Demo)"
)

# Launch the demo in colab — will give a public link
iface.launch(share=True)

